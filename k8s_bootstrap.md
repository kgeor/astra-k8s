Подключитесь по SSH к мастер-узлу и выполните следующую команду для инициализации нового кластера Kubernetes (с заданными параметрами инициализации можно ознакомиться изучив файл */etc/kubernetes/kubeadm-config.yaml*, его содержимое также приведено в сценарии Ansible, который Вы использовали для подготовки узлов кластера)
```
sudo kubeadm init --config /etc/kubernetes/kubeadm-config.yaml
```
Просмотрите вывод kubeadm (это может быть полезно, так как он содержит информацию о том, какие этапы развертывания были пройдены, полученный итоговый результат, а также инструкцию по дальнейшим действиям, которые надо предпринять для получения работоспособного кластера, что впрочем и так будет Вам сообщаться по мере выполнения данной лабораторной работы) и найдите команду для добавления рабочих узлов (не control-plane), скопируйте ее для последующего выполнения на других узлах, она начинается с ```kubeadm join ...```(*Важная информация: в этой команде для авторизации используется временный токен, который действителен 24 часа. При необходимости создать новый токен можно командой ```kubeadm token create --print-join-command```*)

Скопируйте файл конфигурации кластера, чтобы kubectl мог автоматически считывать его (**Запомните, копируемый файл ```admin.conf``` содержит всю необходимую информацию для получения полного доступа к кластеру, его надо беречь**) 
```
cd k8s/
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```
Теперь мы готовы осуществить свое первое взаимодействие с кластером, давайте для начала убедимся что кластер работает и мы имеем доступ к нему, выполнив 
```
kubectl cluster-info
```
Если все в порядке, в выводе Вы получите информацию о статусе и том, по какому адресу (точнее будет сказать, URL) доступны плоскость управления (control plane) кластера (читай API-сервер) и сервис внутрикластерного DNS.

Давайте проверим состояние узлов нашего кластера (состоящего пока что из единственного мастер-узла), сделать это можно командой 
```
kubectl get nodes
```
Узел находится в статусе *NotReady*, однако больше ничего конкретного относительно данной ситуации мы узнать из полученных данных не можем, даже если попросим более подробный вывод в том же табличном формате: ```kubectl get nodes -o wide```. Отложим пока ситуацию с узлом на некоторое время в сторону и попробуем выяснить, какие поды работают (и работают ли) сейчас в кластере. Выполним команду 
```
kubectl get pods
``` 
и получим ответ, что ресурсов (подов в данном случае) в пространстве имен по умолчанию (default namespace) не обнаружено. Так как команде *get* для получения объектов namespace-специфичного ресурса не было передано желаемое пространство имен, то и результат мы получили только для пространства имен по умолчанию, которое сейчас пустует.  Чтобы получить информацию об объектах, существующих в других пространствах имен, их надо указать в запросе kubectl. Дабы понимать, какие у нас пространства имен имеются "из коробки", получим их перечень
```
kubectl get namespaces
```
В выводе помимо уже знакомого default будет интересующее нас в текущий момент *kube-system*, поскольку в нем располагаются объекты, созданные самим кластером, а значит именно там мы должны найти поды с компонентами кластера.

Попробуем еще раз получить информацию о подах, только теперь уже с конкретикой, что мы хотим получить данные из пространства имен *kube-system*
```
kubectl get pods -n kube-system
```
Полученная информация должна нас успокоить в том плане, что компоненты кластера запущены и нормально функционируют, однако отметим, что поды CoreDNS почему-то зависли в состоянии ожидания (Pending), давайте узнаем точную причину происходящего. В этом может помочь команда *kubectl describe*, она используется для того, чтобы получить подробную информацию о конкретном объекте (объектах), в нашем случае о поде CoreDNS. (*Крайне полезная информация: Чтобы нормально читать вывод этой и некоторых других команд в табличном формате, используемом по умолчанию, может потребоваться достаточно сильно увеличить размер окна терминала и возможно понизить размер шрифта*) Полная команда будет выглядеть так (вместо *<pod_name>* подставьте имя любого пода CoreDNS):
```
kubectl describe pods/<pod_name> -n kube-system
``` 
И самый конец вывода, секция События (Events) прямым текстом говорит нам о том, что планировщик (schesduler) не смог найти подходящий для развертывания пода узел по причине того, что на единственном доступном узле висит "черная метка", ограничение (taint), к которому у данного пода нет допуска (toleration). В этом можно убедиться, сравнив пункты из вышележащей секции Tolerations в выведенных данных с вызвавшим проблему ограничением, оно не будет прописано в допусках.

Теперь посмотрим, а как же собственно говоря, получить информацию обо всех однотипных объектах из всех пространств имен кластера сразу на примере получения списка всех существующих на текущий момент в кластере подов:
```
kubectl get pods -A
```
*Полезная информация: узнать какие еще ресурсы кластера существуют помимо ```pod и node```,  можно выполнив команду ```kubectl api-resources``` тут же можно узнать, какие ресурсы namespace-специфичны (как pod), а какие едины для всего кластера (как node)*

Итак, настало время разобраться наконец, что не так с нашим узлом, для этого нам снова поможет команда *describe* для подробного удобочитаемого вывода доступной информации об этом объекте:
```
kubectl describe nodes
```
В ее выводе найдем секцию Conditions, а в ней пункт Ready. Данные в столбце Message достаточно прямолинейно говорят о существующей проблеме: не готова к работе сеть подов, так как не проинициализирован плагин CNI. Еще выше можно найти информацию о текущих ограничениях на узле (секция Taints). Зафиксируйте информацию о том, какое еще ограничение, помимо увиденного в событиях пода CoreDNS ранее, висит на узле (преподаватели могут спросить при сдаче работы) и подумайте, для чего оно существует *(подсказка: все увиденные нами ранее "системные" поды имеют специально прописанный в их спецификациях допуск (толерантность) к этому ограничению, пользовательские нагрузки по умолчанию не имеют допусков)*.

Теперь, когда мы выявили причину всех текущих бед с кластером, надо срочно подумать об установке и настройке CNI-плагина, который отвечает за сеть подов, а если быть точнее, за выделение каждому поду IP адреса, обеспечение связности между подами на разных узлах (посредством маршрутизации либо инкапсуляции) и опционально за применение сетевых политик, разграничивающих доступ. В данной работе будет использоваться один из наиболее известных CNI-плагинов, Calico.

Подробно разобрать принцип работы CNI-плагинов, особенности и тому подобное мы в рамках данной работы не сможем, поэтому ограничимся краткой информацией: CNI-плагин Calico в режиме "по умолчанию" использует для передачи данных инкапсуляцию (туннелирование) IPIP (если кратко, на пакет с заголовком, содержащим IP-адрес из подовой подсети, навешивается еще один IP-заголовок, только уже с адресом узлового интерфейса, выходящего во внешнюю сеть) и динамическую маршрутизацию через протокол BGP с установлением всеми узлами соединений друг с другом (full mesh).

Инкапсуляция крайне важна для случая, когда узлы кластера разнесены по разным IP-сетям и трафик, передаваемый между ними должен маршрутизироваться промежуточными маршрутизаторами (которые в общем случае не в курсе про какие-то внутренние сети кластера из диапазона "серых" IP адресов), в нашем случае "все узлы в одной локальной сети" передача трафика подов между узлами работала бы и без инкапсуляции, поскольку коммутатор не смотрит на IP-заголовки, но менять режим мы не будем. Необходимость в динамической маршрутизации вызывается тем фактом, что единая IP сеть (с маской /16), выделенная для подсети подов в кластере (она задавалась в конфигурации kubeadm как *podSubnet* и потом будет указана в манифесте с параметрами для установки CNI-плагина Calico), разделяется на небольшие подсети (/24), каждая из которых назначается отдельному узлу кластера. И, как нетрудно догадаться, если нужно доставить пакет поду из другой подсети, находящемуся на другом узле, узел, с которого пакет отправляется, должен точно знать, на каком именно узле кластера находится подсеть подов для адреса назначения.

Приступим к установке, для начала установим специальный оператор Kubernetes от разработчиков Calico, который самостоятельно развернет все требуемые компоненты Calico согласно заданным параметрам. Оператор в Kubernetes - это приложение, которое следит за установкой и осуществляет управление (создание и изменение) кастомными (определяемыми с помощью манифестов типа CustomResourceDefinition) ресурсами, помогает отслеживать изменения и поддерживать эти ресурсы в желаемом состоянии. Tigera operator, который мы установим следующей командой, осуществляет полное управление жизненным циклом Calico в кластере k8s, помимо управления кастомными, создает объекты ресурсов стандартного (Deployment, DaemonSet,...) типа, масштабирует контроллеры при необходимости, через него мы можем Calico устанавливать, изменять его конфигурацию, обновлять версию и тд. Следующей командой создадим все необходимые кастомные ресурсы и развернем под (на самом деле под разворачивается не сам по себе, а как экземпляр, прописанный в спецификации создаваемого объекта типа Развертывание (Deployment)), применив манифест, а затем посмотрим на существующие в текущий момент в кластере развертывания (deployments) 
```
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.29.1/manifests/tigera-operator.yaml
kubectl get deployments -A
```
Если теперь мы снова получим перечень всех подов ```kubectl get pods -A```, то обнаружим, что в новом простанстве имен появился новый под *tigera-operator...*. Мы можем подождать, пока он полностью запустится, однако больше никаких изменений не произойдет, проблемы с сетью подов останутся на своих местах, а все дело в том, что пока мы не создадим в кластере объект для кастомного ресурса, соответствующего параметрам установки Calico, оператор ничего делать не будет. Нужная конфигурация была создана заранее на подготовительном этапе и находится в файле *~/k8s/calico.yaml*. Просмотрите содержимое данного файла, наиболее важная информация там - выделенная для подов сеть (CIDR), которая совпадает (и всегда должна совпадать) с указанной *podSubnet* в конфигурации kubeadm для инициализации кластера. Применим манифест (*в отличие от прошлой команды kubectl create, kubectl apply позволяет как создать новые объекты, так и обновить уже существующие*).
```
kubectl apply -f ./calico.yaml
```
Проверим поды ```kubectl get pods -A```, дабы убедиться в том, что помимо прочих появились *calico-kube-controllers..., calico-node...*. Подождите, пока они проинициализируются и запустятся, после этого также должны запуститьтся поды с CoreDNS, а статус узла из вывода ```kubectl get nodes``` должен наконец стать *Ready*. А чтобы окончательно убедиться в том, что сеть подов заработала, давайте посмотрим, получили ли поды свои IP адреса, информация о них доступна при использовании расширенного вывода
```
kubectl get pods -A -o wide
```
В выводе команды можно найти еще и ответ на вопрос, почему некоторые поды успешно работали и до момента внедрения и запуска CNI-плагина (зафиксируйте для отчетности). Ну а раз все работает, значит мы готовы расширить наш кластер, чтобы он мог называться так обоснованно, поэтому приготовьте ранее сохраненную команду для присоединения нового рабочего узла к кластеру, мы начинаем масштабироваться.

В новой вкладке/новом терминале подключитесь к любому будущему рабочему узлу и выполните ранее сохраненную команду для присоединения на нем
```
sudo kubeadm join ...
```
На мастер узле, выведя список узлов и список подов, можем заметить, как некоторые поды автоматически развертываются на новом рабочем узле, перед добавлением второго рабочего узла рекомендуем убедиться, что первый уже перешел в статус *Ready* (ну или Вы при условии недостаточности ресурсов хостовой ВМ рискуете ненадолго уронить мастера).

Добавьте в кластер второй рабочий узел, действия аналогичны первому, дождитесб развертывания на втором узле всех запланированных подов.

Давайте попробуем получить данные о текущей загрузке узлов кластера, у kubectl есть встроенная команда для этого:
```
kubectl top node
```
И... мы получаем ошибку, что Metrics API недоступен. "Из коробки" Kubernetes не содержит компонента для реализации этого функционала, но его можно очень легко установить, давайте посмотрим, как это можно сделать.

До сих пор мы при необходимости что-то создать/установить в кластере использовали kubectl и файлы с YAML-манифестами, однако это не единственный способ, и в некоторых случаях (например, для сложных микросервисных приложений) точно не самый простой. Для Kubernetes существует менеджер пакетов, который подобно пакетному менеджеру Apt для Debian-подобных дистрибутивов Linux, значительно упрощает развертывание, обновление и обслуживание приложений в кластере, используя чарты (приблизительно это шаблоны требуемых для развертывания приложения манифестов, по примеру тех шаблонов, что Вы использовали для файлов конфигурации сервисов в работе, посвященной Ansible), хранимые в общедоступных репозиториях. 

Установим сам Helm
```
curl -L https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash
```
Для удобства сразу настроим автодополнение вводимых команд (по нажатию Tab), как было на этапе подготовки сделано для kubectl (а Вы еще не пробовали?)
```
source <(helm completion bash)
helm completion bash | sudo tee /etc/bash_completion.d/helm > /dev/null
```
Ппробуем установить теперь с его помощью компонент, реализующий Metrics API - Metrics server, который позволит получать данные о потребляемых ресурсов подами и загрузке узлов используя команду ```kubectl top``` (а также необходим для функционирования горизонтального автомасштабирования нагрузок), но для начала добавим нужный репозиторий и скачаем файл, содержащий значения параметров установки, поскольку потребуется внести некоторые изменения.
```
helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server
helm repo update
helm show values metrics-server/metrics-server > ./metrics-server.values
```
По умолчанию Metrics server требует, чтобы в кластере функционировал выпуск X.509 сертификатов от имени доверенного удостоверяющего центра (Certification Authority), иначе он не будет подключаться к узлам с недоверенными сертификатами. Чтобы исправить эту ситуацию, мы можем немного подправить его конфигурацию, для этого мы специально последней выполненной командой сохранили все используемые при установке Metrics server параметры (пока что имеющие значения по умолчанию) и теперь мы можем по своему усмотрению подправить этот аналог файла с переменными для сценария Ansible. В файле *metrics-server.values* найдите приведенный ниже фрагмент текста и добавьте последний, отсутствующий по умолчанию параметр
```
defaultArgs:
  - --cert-dir=/tmp
  - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
  - --kubelet-use-node-status-port
  - --metric-resolution=15s
  - --kubelet-insecure-tls
```
Теперь все готово к установке, осталось только создать выделенное пространство имен и можно приступать
```
kubectl create ns metrics-server
helm install metrics-server metrics-server/metrics-server -n metrics-server --values ./metrics-server.values 
```
После того, как все поды в пространстве имен *metrics-server* запустятся, можете попробовать получить данные о загрузке узлов
```
kubectl top node
```
*Если возвращается ошибка, немного подождите, сбор метрик при первом запуске Metrics server происходит не быстро*

На пути к полностью готовому к развертыванию пользовательских приложений кластеру нам остался один шаг, заключающийся в демонстрировании возможности предоставления постоянного хранилища для развернутых в кластере Stateful-приложений, которые, как известно, должны сохранять свои данные между перезапусками. По аналогии с Docker, хранилища, монтируемые внутрь контейнера в поде называются томами (volume). Том в контексте Kubernetes может быть обычным (volume), который определяется внутри спецификации пода и его жизненный цикл целиком завязан на под (т.е. удалится под, удалится и том), либо постоянным (PersistentVolume). Объекты типа PersistentVolume (PV) имеют свой, независимый от других объектов жизненный цикл, их можно подключать к нескольким контейнерам сразу при указании сответствующего типа доступа. Для подключения томов мы можем использовать как встроенные в k8s драйвера некоторых типов хранилищ, и самостоятельно вручную определять каждый том отдельным манифестом, без необходимости доустанавливать что-либо, так и использовать автоматическое выделение тома соответствующего класса хранилища (StorageClass) по заявке PersistentVolumeClaim (PVC), используя встроенный либо внешний поставщик (Provisioner) и при необходимости, внешний CSI-драйвер.

Далее мы продемонстрируем оба упомянутых варианта. В качестве хранилища будем использовать NFS сервер на хостовой ВМ. (*Краткая справка, NFS (Network File System) - это в некотором роде аналог SMB протокола, позволяющий осуществлять доступ на файловом уровне к директориям на сервере*)
Давайте первоначально его установим, для этого потребуется запустить соответствующий сценарий Ansible на хостовой ВМ и создать вручную директорию для статически выделенного тома.
```
ansible-playbook ./nfs_host.yml
mkdir /srv/nfs_share/pv1
```
Хранилище для томов PV готово, теперь вернемся на мастер узел и создадим первую заявку PersistentVolumeClaim
```
cat << EOF > test-pvc.yaml
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: nfs-claim-manual
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
EOF
kubectl apply -f test-pvc.yaml
```
Проверим состояние заявки и наличие томов PV в кластере
```
kubectl get pvc
kubectl get pv
```
Заявка PVC находится в состоянии *Pending* (посмотрите в подробном выводе состояния этого pvc, почему так, зафиксируйте для отчетности) и похоже, поскольку тома PV сейчас в кластере отсутствуют, подходящий том нам придется создать самостоятельно, применив следующий манифест. Параметр *persistentVolumeReclaimPolicy* отвечает за судьбу тома PV, после того, как сцепленная с ним заявка PVC будет удалена. В приведенном случае том останется как есть (Retain).
```
cat << EOF > test-pv.yaml
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-host-pv1
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  nfs:
    server: 192.168.122.1
    path: /srv/nfs_share/pv1
EOF
kubectl apply -f test-pv.yaml
```
Подождем секунд 5-10 и снова проверим, что том создался и успешно "сцепился" с заявкой PVC.
```
kubectl get pvc
kubectl get pv -A
```
Если все прошло как планировалось, то в выводе информации о PVC и PV статус обоих будет *Bound*, кроме того для тома PV в столбце CLAIM будет указано, с какой именно заявкой PVC он сцеплен.

Для реализации варианта автоматизированного управления томами PV, нам потребуется установить внешний поставщик (Provisioner) для NFS (так как для NFS Kubernetes не имеет встроенного поставщика), который в директории, экспортируемой с сервера NFS, будет создавать по вложенной директории на каждый создаваемый том PV.
Установим его, снова используя Helm, при установке также автоматически создастся класс хранилища по умолчанию (он используется, если желаемый класс не указан в заявке PVC)
```
helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/
kubectl create ns nfs-provisioner
helm -n nfs-provisioner install nfs-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \
    --set nfs.server=192.168.122.1 \
    --set nfs.path=/srv/nfs_share \
    --set storageClass.defaultClass=true \
    --set replicaCount=1 \
    --set storageClass.name=nfs \
    --set storageClass.provisionerName=nfs-provisioner 
```
После развертывания экспресс-проверку работоспособности можно провести так:
Создаем еще одну заявку PersistentVolumeClaim на том PersistentVolume
```
cat << EOF | kubectl apply -f -
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: nfs-claim-auto
spec:
  storageClassName: nfs
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
EOF
```
Проверим состояние созданной заявки и убедимся, что она сразу перешла в статус *Bound*, а среди томов PV появился новенький
```
kubectl get pvc/nfs-claim-auto
kubectl get pv
```
Поскольку том создавал поставщик, а не мы сами, то и параметры, с которыми он был создан, для нас не очень ясны, к счастью имеется способ для уже существующего в кластере объекта получить его YAML-манифест (с поправкой на присутствие в выводе дополнительных полей, вроде статуса, метки времени изменения и тп), делается это указанием YAML как желаемого формата вывода
```
kubectl get pv/<pv_name> -o yaml
```
На основе предыдущего вывода, зафиксируйте, какую *persistentVolumeReclaimPolicy* имеет созданный поставщиком том. 
И наконец настало время финальной проверки, создаем для каждого тома PV под, задача которого - в директории, куда смонтирован том, создать файл, чтобы мы смогли удостовериться, что данные из пода успешно сохраняются на нашем NFS сервере.
```
for method in auto manual; do
cat <<- "EOF" | kubectl apply -f -
kind: Pod
apiVersion: v1
metadata:
  name: nfs-pv-$method
spec:
  containers:
  - name: nfs-test
    image: busybox:stable
    command:
      - "/bin/sh"
    args:
      - "-c"
      - "touch /mnt/SUCCESS-$(hostname) && echo \"Successfully created the file SUCCESS-$(hostname) ;)\" && exit 0 || echo \"couldn't create the file :(\" && exit 1"
    volumeMounts:
      - name: nfs-pvc
        mountPath: "/mnt"
  restartPolicy: "Never"
  volumes:
    - name: nfs-pvc
      persistentVolumeClaim:
        claimName: nfs-claim-$method
EOF
done
```
Контейнер в поде при успешном создании файла (впрочем, при ошибке создания тоже) выводит запись о результате в свой STDOUT, что является стандартным способом логирования для развертываемых в k8s контейнеризованных приложений (*можете потом самостоятельно по аналогии с приведенной ниже командой просмотреть логи любого другого пода*). Для просмотра логов пода, существует отдельная команда kubectl - *logs*, подобно аналогичной для docker. Давайте возьмем для примера один из подов и убедимся по логам, что контейнер в нем справился с задачей, оставив запись об успехе ее выполнения:
```
kubectl logs pods nfs-pv-...
```
Полезно будет отметить, что поскольку контейнеры в этих двух подах были рассчитаны на выполнение конкретного действия и последующее завершение работы, то при просмотре их состояния, поды ```nfs-pv...``` будут в статусе *Completed*, что также говорит об успешном выполнении запущенной в контейнерах команды (код возврата 0). Если хотите окончательно убедиться в работоспособности хранилища, на хостовой ВМ внутри директории */srv/nfs_share* можете найти внутри созданной ранее вручную pv1 и автоматически созданной поставщиком директорий файл вида SUCCESS-... .
